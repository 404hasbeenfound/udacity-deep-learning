{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "1562484\n",
      "26\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.next()[1].shape)\n",
    "print(len(train_text) // batch_size)\n",
    "print(len(string.ascii_lowercase))\n",
    "print(np.zeros(shape=(2, 4), dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298057 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "zvwodumhctquapdore hgtmsgusgaywtkcexo  mer    n synyi ck ntsafeji trroma takbssq\n",
      "oys rifvzioknbfr udsgrwseond vohsaoos knbsl hrar dsttxtroyfaxto sslusjohy edeqro\n",
      "fcx lasizajldkbnffdpnwqj wo aiabhdb sysxaskilqsgy hepbhknuwsveafgnwvrad btlfrquz\n",
      "euyrrsa alqkjt eueirtkeu gj xtmvngcjsel ogmlaynfe fojtenkbeeeqyoe txr mysk cze a\n",
      "nzfxrre ob osxeoo isdn udyzisel iujtrdidxziogzegtaone s hhcaqntodlndaglklyacen h\n",
      "================================================================================\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.588228 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 10.11\n",
      "Validation set perplexity: 10.70\n",
      "Average loss at step 200: 2.247158 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 9.51\n",
      "Validation set perplexity: 9.29\n",
      "Average loss at step 300: 2.096215 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 2.000496 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 500: 1.933016 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 600: 1.908125 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 700: 1.860565 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.819593 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 900: 1.832508 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000: 1.827426 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "uctist butiss time c s actern conf campible fign centres ulinum chirusion knakai\n",
      "k and bimks litt is two one six gelect diatorition from semeni s ras enrimanly d\n",
      "kown u us of kinclurm frongdrencek as chace us inding becaxting meriduised an ap\n",
      "l kight constriscaity diviscliyy forlinction of the jecints tradion fix distrofi\n",
      "ning foller sutelf he warct brited whar courch how the high hay and s happersm w\n",
      "================================================================================\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1100: 1.780429 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1200: 1.756203 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.732704 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1400: 1.746610 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1500: 1.744732 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1600: 1.747744 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1700: 1.715083 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.685281 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.653724 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2000: 1.700770 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "reed pronucy hiscused perpodience on a mocthour plantisu ninean and dviners bref\n",
      "thoutther ore fiam the citwer they a utible twher a onf three equs aso d mozer l\n",
      "recthxeen beading may the chorshove smeen scruin ediever the serdence on esmiale\n",
      "ber the landles is filmsil notion malament ia lat four five tine for deed in thi\n",
      "ain stermudoope six nine lut veptern houpaniscave uterked the duld bi play i one\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2100: 1.690205 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.686660 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2300: 1.645279 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.664887 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2500: 1.680605 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2600: 1.655120 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.661015 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2800: 1.652530 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.646658 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3000: 1.651464 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "jects one nine three jaginin two tedson s not the bly thought decake as in boomo\n",
      "old so other gover in of the uncrextross cynta plarde to they butly six smshds t\n",
      " de its revisic pracks in vairity ofqecile beton to following com lesition gramm\n",
      "wels puble papitice paple par retray basi him from orecsed techn known  dob dimo\n",
      "one in jay all brickhnec incterious harged and and randel race rejovive two zero\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3100: 1.631000 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3200: 1.644679 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.636841 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.670441 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3500: 1.658002 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.671428 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.646906 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3800: 1.644250 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3900: 1.638899 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.649493 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "y caming all aftime regownine brinc laterdes in protean eight jawy lams marshust\n",
      "varians staltsion thought has the pocus the jousli anuring insequent in unsope o\n",
      "x chrict of most or the sand to first in phathabll ras to languably also the bot\n",
      "k saided hone allim thoush in this hock entroquated to rinomon games thoen ita a\n",
      "s acjorals entrolace riagied by his dimising the ross were his inadistal erfinat\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4100: 1.635109 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.635807 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.623295 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4400: 1.612835 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.616055 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4600: 1.616270 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.626915 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4800: 1.631688 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.634020 learning rate: 10.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5000: 1.606020 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.46\n",
      "================================================================================\n",
      "ate a cline du immersity masely one eight foreh equility for the librations ibac\n",
      "der order is from with h deld invalsion thin in the more primits bares as well o\n",
      "pies a revent batcher is thromm shar whate the one two cincled in of they shipti\n",
      "mberton repce of foneh firitooy one nine two ts or is the romantal life blackne \n",
      "king s poculony locisonalatist rained to between gash pulio have two zero z wase\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.606998 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.592710 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.587298 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5400: 1.580442 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.569875 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.583284 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5700: 1.571971 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.581509 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5900: 1.575152 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6000: 1.551735 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "j distany corncional for the recedention in veried as sonnor between the printic\n",
      "x skitherar and the temperationsh to five actroxe in vezibal liats is numbers se\n",
      "ting set despect back this where desond s city bood sebrac dew commaniars in s a\n",
      "wine an set sciental the cubrdied muntail and yundly right is rigesices karebar \n",
      "il exime through film is actifial contelfed ais evil allaw capsist ancient of de\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6100: 1.566910 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6200: 1.534930 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.550266 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6400: 1.545666 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6500: 1.562405 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6600: 1.598086 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6700: 1.578470 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6800: 1.606102 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6900: 1.583614 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7000: 1.579345 learning rate: 1.000000\n",
      "640\n",
      "640\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "her orders kennbial pusphacfer to bewirs ofpoces from masery who he heiria varai\n",
      "p bords on after in this noo allo a both gajotack theory datlilest in asmace eig\n",
      "s one six east ward adizm one nine one five milled in the refersed flenching use\n",
      "dest arly d i veenation of arenach the haw echam store dreach definition absurba\n",
      "speat in writer invalap the n eight medision the phanish moder hindowy at the tr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print(len(labels))\n",
    "      print(len(predictions))\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297247 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      " xynieneieduenqdvteremkpixmiisfweor zedn nguflrnhnnnpsaem zu lkv amj ekunzbw zld\n",
      "picrie tvvh wsbumauneeeooebwixledbwjvhgf s  irnd reebtxnvivycyfwvwtqyydmnvbxtyuu\n",
      "akweozrsspgmyvofnekze wiisi mriwv dshtk skpeti oul ais gjaaeapnaemggczxuiiaj vss\n",
      "onq beh xu nov unk urcmr avfd ygoe mfycthomfxmzjc cvkteqmpsxgke b scqcbnngscwrut\n",
      "s tqybcfeejkeva   sh  aksle etaym rwtoscxvixjbf foewmp lpheetieomfcwacoehmhitreq\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.588999 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.24\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.246476 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.37\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 300: 2.086178 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.033016 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.975189 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.892246 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 700: 1.868190 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 800: 1.861976 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 900: 1.839710 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.836722 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "vemsil uslyse the v unalable livder ow wlot farms uj have e chouttulited to aman\n",
      "uc it bateralbwis phriem levurlin d caizsation arkitism ifilingsan in the lingi \n",
      "quoin achasinss egenty used to hists who prom cossion lastones she sth himbulled\n",
      "xing acairents s more memisifent stanc redate whild thoukifi witree in mpvoudide\n",
      "neded with moone diphil karusn uspertate homes this meating the ffor waadity hav\n",
      "================================================================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1100: 1.792909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.764628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.753803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1400: 1.757464 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.740022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1600: 1.725737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1700: 1.711830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1800: 1.687133 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1900: 1.693027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.676565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "ach toncereen tiste intl one nine seven allougen lare it dreen securures statisu\n",
      "fold nor eavily of the abourlaralralisfe and notmally intearthed boddoraihe in t\n",
      "y camed recovery math ortectioniciage teblitter stardel thesued in the complemen\n",
      "y of one nine nine nine one six by nematipele conducs collige were no was afteem\n",
      "d breathian pant poiters forces to achold overs othan linced for levion finee te\n",
      "================================================================================\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2100: 1.683243 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2200: 1.699659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.704284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.679653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2500: 1.688270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2600: 1.667778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2700: 1.686231 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2800: 1.679804 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2900: 1.675186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3000: 1.679133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "ani of the empressians of wrexo guired for a they pinnet them is notickly ass as\n",
      "rang invents of ramist ther be harm the drunchoje of technort out one nine binat\n",
      "quic two  sugct a crimmaties to besinament frence two fivel sour two zero m s to\n",
      "d the fed typoske wishormario flow wares of stavistabs or elsted de kurgeting pe\n",
      "lectter marda ilage xook pan sequences as chilist leating a perrenctal partater \n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3100: 1.650980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3200: 1.633125 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3300: 1.641497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3400: 1.633213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3500: 1.671292 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3600: 1.650366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3700: 1.652233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3800: 1.659988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3900: 1.653002 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 4000: 1.640062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "d theig also boprowt speens e c four flacks for two zero zero nine in throe the \n",
      "prole ass and be other two five one eight four four four eight one conforce anno\n",
      " prosomphonaly towarits of there and cancithed thirds from full of a condects of\n",
      "ding to name rume as a destandal proprace econdeder the has our beporngue it mas\n",
      "de infleeal thin by speet to that procrable of the intorstic variad new marker y\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4100: 1.613630 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4200: 1.614082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4300: 1.617772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.610407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4500: 1.641903 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4600: 1.625347 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4700: 1.621578 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4800: 1.601425 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4900: 1.618741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5000: 1.608514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "veles yoush four a nomisreist nation id ontex sketing obdep and one nine seven o\n",
      "ngoo more musicularsic hateregors cont or used to the leatismmg wutheibers engli\n",
      "quisbal kirg esppertabully tupting to chindlo p four imations may one nine six t\n",
      "mpte cublism juch to variant it to a create b hayh the the its lokes on mif ery \n",
      "vy zounch e f bee of musicult grouper one nine seven two one it i three menicati\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5100: 1.591485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5200: 1.590663 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5300: 1.593313 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5400: 1.592484 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5500: 1.590597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5600: 1.563452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5700: 1.581077 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.597078 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5900: 1.582498 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6000: 1.584490 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "may periopted usually thomay fead alnowy with the chinis in one zero s re posity\n",
      "ine from appeiragia drectional influence of and however one weve ti case ministe\n",
      "yase scounding his in despsic philwed thoun posial tripan reconnots in the sever\n",
      "peed have spire scoinid by the curred and incill or vess foll and the bethce tro\n",
      "chating the one nos the as a long the man to grequally to the sands over are meg\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6100: 1.577308 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6200: 1.586001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6300: 1.586054 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6400: 1.573925 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6500: 1.556049 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6600: 1.603063 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6700: 1.566264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6800: 1.580129 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6900: 1.571074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 7000: 1.587807 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "ing first tenning the man younges and supply in vourol foundenal camined on popu\n",
      "guart from two erternal in the naturay used dertailes called a sala counted feat\n",
      "jaui for its casters and gecing alphs has gio to perforce sance detealed in adri\n",
      "warturi gayed to one eight kastics whiledsto beadniscic supple cir priver other \n",
      "joles and haivebla work nine eight four boim id the repreedent it as an ingovesi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, remove the 1-hot encoding on the input and replace it with char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator2(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      #batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters2(batch):\n",
    "  \"\"\"Turn a batch over the possible\n",
    "  characters back into its character representation.\"\"\"\n",
    "  return [id2char(c) for c in batch]\n",
    "\n",
    "#TODO: chararcters2 with argmax probability\n",
    "\n",
    "def batches2string2(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters2(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator2(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator2(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string2(train_batches.next()))\n",
    "print(batches2string2(train_batches.next()))\n",
    "print(batches2string2(valid_batches.next()))\n",
    "print(batches2string2(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches.next()\n",
    "print (batches[1].shape)\n",
    "# Should be (64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([embedding_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,]))\n",
    "  embeds = list()\n",
    "  for i in range(num_unrollings + 1):\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, train_data[i])\n",
    "    embeds.append(embed)\n",
    "  train_inputs = embeds[:num_unrollings]\n",
    "  train_labels = embeds[1:] # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  norm_predict = tf.sqrt(tf.reduce_sum(tf.square(logits), 1, keep_dims=True))\n",
    "  normalized_predict_embeddings = logits / norm_predict\n",
    "  norm_vocabulary = tf.sqrt(tf.reduce_sum(tf.square(vocabulary_embeddings), 1, keep_dims=True))\n",
    "  normalized_vocabulary_embeddings = vocabulary_embeddings / norm_vocabulary  \n",
    "  similarity = tf.matmul(normalized_predict_embeddings, tf.transpose(normalized_vocabulary_embeddings))\n",
    "  train_prediction = tf.nn.softmax(similarity)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 1.476091 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.54\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 27) for Tensor u'Placeholder_11:0', which has shape '(1, 128)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-611d7dae505b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m           \u001b[0mreset_sample_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m79\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0msentence\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/arnaud/Documents/Python/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \"\"\"\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/arnaud/Documents/Python/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3496\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3497\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3498\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/arnaud/Documents/Python/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/arnaud/Documents/Python/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    626\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 27) for Tensor u'Placeholder_11:0', which has shape '(1, 128)'"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      labels_1h = np.zeros(shape=(len(labels),vocabulary_size), dtype=np.int)\n",
    "      for b in range(len(labels)):\n",
    "        labels_1h[b, labels[b]] = 1.0\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels_1h))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
