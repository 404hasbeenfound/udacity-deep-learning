{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 27)\n",
      "1562484\n",
      "26\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_batches.next()[1].shape)\n",
    "print(len(train_text) // batch_size)\n",
    "print(len(string.ascii_lowercase))\n",
    "print(np.zeros(shape=(2, 4), dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292894 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "[[ 0.03726107  0.03669411  0.03623831 ...,  0.03794174  0.03803772\n",
      "   0.0375132 ]\n",
      " [ 0.03718907  0.03688489  0.03590617 ...,  0.03765091  0.03719876\n",
      "   0.03672199]\n",
      " [ 0.03694923  0.03718112  0.03600221 ...,  0.03763348  0.03791139\n",
      "   0.0364771 ]\n",
      " ..., \n",
      " [ 0.03789713  0.03654435  0.03607545 ...,  0.03819678  0.03802063\n",
      "   0.03825819]\n",
      " [ 0.03651469  0.03753807  0.03821511 ...,  0.03687868  0.03667106\n",
      "   0.03703224]\n",
      " [ 0.0384055   0.03683623  0.03705533 ...,  0.03816783  0.03857732\n",
      "   0.03694431]]\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "phste mpfgga ihwmkhtkgnnserp qtynglea zeubmssgp ua ol fqmwxrfrxjaopie jwmgwrfsne\n",
      "q ctlggqoiotp ltpjw ihqamioqpnirtisljmj eunkterr  tuifnueeurn t nr inozgtibduayn\n",
      "xxsqgtrdn eqo otpig hpcizls qllngsbieytlr talcjlskiocee ix h horp phnoptvtdqnrol\n",
      "sjf advx bk aerv oneph etpjkuxzxhahyhigz zoq uicttayml lotyapicemwxes tra d esoo\n",
      "qelyq onjjntrganiqqyborvhmjwnheelgw aivyensknlorhdtxmemyehctc tpfcn bejopmdgoyce\n",
      "================================================================================\n",
      "Validation set perplexity: 20.04\n",
      "Average loss at step 100: 2.585935 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.01652475  0.08918072  0.02932908 ...,  0.00301912  0.00571306\n",
      "   0.01315291]\n",
      " [ 0.06986282  0.01909774  0.01313754 ...,  0.00276867  0.01336831\n",
      "   0.00307198]\n",
      " [ 0.08504652  0.03972001  0.01171961 ...,  0.00747614  0.00434835\n",
      "   0.00401916]\n",
      " ..., \n",
      " [ 0.07954237  0.01554471  0.01188331 ...,  0.003746    0.0073305\n",
      "   0.00344988]\n",
      " [ 0.00319587  0.08997932  0.0483916  ...,  0.00185998  0.00387934\n",
      "   0.01805664]\n",
      " [ 0.16199079  0.06446671  0.006386   ...,  0.00233965  0.02230896\n",
      "   0.00313553]]\n",
      "Minibatch perplexity: 10.10\n",
      "Validation set perplexity: 10.58\n",
      "Average loss at step 200: 2.241320 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.36208677e-03   1.74373925e-01   2.92887576e-02 ...,   1.78697368e-03\n",
      "    6.63782191e-03   7.86323193e-03]\n",
      " [  1.22119964e-03   2.96029206e-02   1.35191818e-04 ...,   1.62744647e-04\n",
      "    6.03593793e-03   9.69275061e-05]\n",
      " [  8.43418896e-01   5.88891655e-03   3.85583553e-04 ...,   5.74999081e-04\n",
      "    4.81576019e-04   1.66650731e-04]\n",
      " ..., \n",
      " [  7.38311466e-03   3.41532752e-02   2.46502496e-02 ...,   6.77797711e-03\n",
      "    6.03667041e-03   3.28615378e-03]\n",
      " [  1.29151242e-02   3.53686400e-02   8.14929663e-04 ...,   6.65008265e-04\n",
      "    1.07562263e-02   7.47308193e-04]\n",
      " [  1.50184566e-02   1.55032575e-02   2.83721928e-02 ...,   6.61226967e-03\n",
      "    1.17484322e-02   3.07771191e-03]]\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 300: 2.090829 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.51497221e-03   6.63664341e-02   1.92874853e-04 ...,   4.12842201e-05\n",
      "    3.41238477e-03   9.60917714e-06]\n",
      " [  1.97637966e-03   1.37460798e-01   1.11170555e-03 ...,   1.93002968e-04\n",
      "    2.04662476e-02   9.40997779e-05]\n",
      " [  2.63098773e-04   2.22720243e-02   2.62404513e-02 ...,   4.90507693e-04\n",
      "    1.51702657e-03   3.48044574e-01]\n",
      " ..., \n",
      " [  1.31619260e-01   2.02495642e-02   1.87600276e-03 ...,   1.13220885e-02\n",
      "    6.28508534e-03   1.00911700e-03]\n",
      " [  2.97121033e-02   8.36913958e-02   5.43484837e-03 ...,   1.20990619e-03\n",
      "    1.94597263e-02   1.66672806e-03]\n",
      " [  1.26601577e-01   5.07318228e-02   4.27787798e-03 ...,   1.45004626e-04\n",
      "    5.58027718e-03   4.44977806e-04]]\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 400: 2.000487 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.95121337e-02   5.64394658e-03   2.21237708e-02 ...,   7.54253613e-03\n",
      "    6.80225622e-03   2.09790259e-03]\n",
      " [  1.52647337e-02   7.72787482e-02   2.46206508e-03 ...,   9.85308290e-02\n",
      "    2.81570852e-03   2.01909640e-03]\n",
      " [  1.40364543e-01   2.99596712e-02   3.68550466e-03 ...,   6.32717420e-05\n",
      "    3.84560670e-03   1.18922407e-03]\n",
      " ..., \n",
      " [  1.96973165e-03   1.18619226e-01   5.33873681e-04 ...,   5.97011858e-05\n",
      "    1.23480558e-02   2.64825412e-05]\n",
      " [  1.11217231e-01   4.45879437e-03   1.01829004e-02 ...,   2.70601711e-04\n",
      "    1.28484210e-02   1.60449941e-03]\n",
      " [  5.87148741e-02   6.90809190e-02   1.52968278e-04 ...,   1.80734685e-04\n",
      "    8.66991002e-03   3.41316278e-04]]\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 500: 1.934718 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.33789045e-01   3.46775651e-02   4.15572524e-03 ...,   3.69357818e-04\n",
      "    1.63220451e-03   2.20078262e-04]\n",
      " [  7.25157652e-03   1.83060557e-01   7.63560506e-03 ...,   2.78913276e-03\n",
      "    1.20032951e-02   2.13052928e-02]\n",
      " [  1.83192897e-04   9.38697830e-02   4.15557027e-02 ...,   3.21334403e-04\n",
      "    4.47619386e-04   3.45277297e-03]\n",
      " ..., \n",
      " [  2.73663946e-03   9.34477448e-02   5.83185181e-02 ...,   2.45426025e-04\n",
      "    1.78452593e-03   4.03817929e-03]\n",
      " [  2.96124399e-01   4.42919880e-02   9.45363019e-04 ...,   3.29855393e-05\n",
      "    1.21708401e-02   1.19762699e-04]\n",
      " [  2.19172448e-01   9.73666366e-03   8.16907501e-04 ...,   8.70519970e-03\n",
      "    1.10704610e-02   1.62757654e-03]]\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 600: 1.906661 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  5.56784421e-02   1.23053463e-02   1.98657042e-03 ...,   1.84947043e-04\n",
      "    3.21553508e-03   3.27948597e-04]\n",
      " [  5.25796739e-03   1.16645291e-01   2.77649262e-03 ...,   1.17045405e-04\n",
      "    2.07936317e-02   6.58584572e-03]\n",
      " [  1.68375093e-02   6.72375038e-03   1.39828362e-02 ...,   1.59554719e-03\n",
      "    3.26227285e-02   1.02563733e-02]\n",
      " ..., \n",
      " [  9.53814745e-01   6.30243041e-04   2.66658782e-04 ...,   2.34562740e-05\n",
      "    3.87229047e-05   9.19246213e-06]\n",
      " [  9.18881834e-01   2.14236788e-03   1.12826870e-04 ...,   8.80455555e-05\n",
      "    6.80722622e-03   7.11456141e-06]\n",
      " [  1.98472813e-01   2.52335034e-02   5.17592207e-03 ...,   5.19891582e-05\n",
      "    7.43702380e-03   5.25841373e-04]]\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 700: 1.858480 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.24202386e-04   1.66036189e-02   1.04753403e-02 ...,   7.10640922e-02\n",
      "    7.55276124e-04   8.41934048e-03]\n",
      " [  7.25250244e-01   1.74133421e-03   1.14979816e-03 ...,   2.78256572e-04\n",
      "    8.56899424e-04   6.01015927e-04]\n",
      " [  1.11770563e-01   8.69619474e-02   4.07832442e-04 ...,   5.73435645e-05\n",
      "    2.94669420e-02   2.41623289e-04]\n",
      " ..., \n",
      " [  2.10155314e-03   1.19917281e-03   3.85616841e-05 ...,   7.45168222e-07\n",
      "    7.54075451e-03   3.30960597e-06]\n",
      " [  1.38630882e-01   1.48258675e-02   3.14001692e-03 ...,   5.21567599e-05\n",
      "    4.26489580e-03   4.51941480e-04]\n",
      " [  1.16554946e-02   1.59973942e-03   5.05160652e-02 ...,   2.07937905e-03\n",
      "    1.41747016e-02   5.11482311e-03]]\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.824811 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.80660808e-01   1.06323659e-02   1.49109971e-03 ...,   3.27469133e-05\n",
      "    3.20606586e-03   1.21242600e-04]\n",
      " [  3.92865110e-03   6.20633624e-02   4.30264771e-02 ...,   1.62110548e-03\n",
      "    2.60917959e-03   3.77283245e-03]\n",
      " [  1.50906143e-03   1.15417108e-01   4.20163833e-02 ...,   4.94872918e-04\n",
      "    1.63760269e-03   3.53303971e-03]\n",
      " ..., \n",
      " [  2.07198225e-02   1.38548493e-01   2.41164118e-03 ...,   1.22748379e-05\n",
      "    4.12712581e-02   1.41078344e-04]\n",
      " [  1.19999692e-01   2.37392157e-01   2.22527445e-03 ...,   1.30713190e-04\n",
      "    4.20288695e-03   3.13682482e-04]\n",
      " [  4.14713740e-01   4.05410081e-02   1.78908873e-02 ...,   1.00154248e-04\n",
      "    4.02185717e-04   9.31271861e-05]]\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 900: 1.834174 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.81592304e-01   4.09215875e-02   5.32388454e-03 ...,   8.04397103e-04\n",
      "    8.32747575e-03   1.44479750e-03]\n",
      " [  6.36217883e-03   2.00487743e-03   9.18609556e-03 ...,   3.96944350e-04\n",
      "    3.05010867e-03   2.58849980e-03]\n",
      " [  1.10811600e-02   3.71536240e-02   1.10145258e-02 ...,   4.38900571e-03\n",
      "    1.60490803e-03   5.15130674e-03]\n",
      " ..., \n",
      " [  4.16389405e-04   1.01691820e-02   5.86921014e-05 ...,   2.01930052e-05\n",
      "    7.61420466e-03   4.14851893e-05]\n",
      " [  5.01855451e-04   7.22672418e-02   4.75418530e-02 ...,   5.11292659e-04\n",
      "    6.43812818e-03   2.15749200e-02]\n",
      " [  8.85020941e-03   1.62608735e-03   2.66528409e-03 ...,   6.26731664e-04\n",
      "    6.95812487e-05   1.67730890e-04]]\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1000: 1.823167 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.47710109e-01   2.38838848e-02   7.66792044e-04 ...,   1.43619873e-05\n",
      "    1.15965754e-02   1.09540953e-04]\n",
      " [  4.76394827e-03   5.00682443e-02   1.02352307e-04 ...,   1.47312778e-04\n",
      "    4.17123176e-03   1.16877800e-05]\n",
      " [  7.91140459e-03   1.46213546e-01   6.69122965e-04 ...,   2.74177182e-05\n",
      "    1.44462427e-02   3.29971954e-05]\n",
      " ..., \n",
      " [  1.78196782e-03   9.59310010e-02   4.67314967e-05 ...,   3.14935351e-05\n",
      "    4.16466472e-04   5.07306959e-06]\n",
      " [  5.15572865e-05   1.00334478e-03   4.37943032e-04 ...,   9.64329112e-03\n",
      "    4.99010785e-05   3.39650922e-03]\n",
      " [  1.15622105e-02   1.02226689e-01   3.91629199e-03 ...,   1.93317067e-02\n",
      "    4.98786289e-03   5.29803801e-03]]\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "in haselo firms their rinss ameling cherd dnuth and factras of contexted section\n",
      "vist cluage which of musiblee the guncal nount of a sown whiphwhit ks stutelmtio\n",
      "x at a dustinal and cuss and adinds hists with mister pearlind reticle blect the\n",
      "was nove eequelled mothenter lip in hen i minizang sien dayion unday or inding s\n",
      "kenting for muy the exille an beatlo theyssidated tine sipprers hin their engena\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.778346 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.79667935e-01   2.30858140e-02   5.25455968e-03 ...,   5.81811881e-04\n",
      "    9.71268937e-02   7.12726731e-03]\n",
      " [  9.35061932e-01   8.08005920e-04   2.99560692e-04 ...,   5.77695346e-06\n",
      "    7.94456981e-04   7.22342083e-05]\n",
      " [  8.89338329e-02   7.64378114e-04   1.87610630e-02 ...,   1.37503853e-03\n",
      "    9.80495708e-04   2.37568907e-04]\n",
      " ..., \n",
      " [  5.28660254e-04   6.12577470e-03   4.33177361e-03 ...,   7.26451923e-04\n",
      "    9.83230129e-05   1.30482926e-03]\n",
      " [  1.18246032e-02   1.13582931e-01   2.33129319e-02 ...,   3.43779830e-05\n",
      "    7.24513317e-04   2.84790091e-04]\n",
      " [  1.84779048e-01   9.06312019e-02   4.96602792e-04 ...,   1.55130747e-05\n",
      "    3.83555293e-02   5.14767307e-04]]\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1200: 1.754154 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.10953208e-03   1.33490153e-02   4.36289629e-05 ...,   1.60900308e-05\n",
      "    1.99435512e-03   8.27906188e-05]\n",
      " [  2.87171062e-02   5.79703860e-02   1.71213988e-02 ...,   2.23778724e-03\n",
      "    5.92910219e-04   4.78441967e-03]\n",
      " [  7.98386514e-01   1.25740571e-02   1.00318073e-04 ...,   1.76377544e-05\n",
      "    1.10596521e-02   1.17511991e-04]\n",
      " ..., \n",
      " [  9.76100042e-02   2.71299928e-01   1.25862556e-02 ...,   2.57804677e-05\n",
      "    2.97834259e-03   5.41852030e-04]\n",
      " [  1.06480531e-03   1.15351476e-01   3.79959606e-02 ...,   5.75141865e-04\n",
      "    1.93524256e-03   2.62421276e-03]\n",
      " [  6.73118770e-01   9.82185593e-04   5.45997813e-04 ...,   9.65666623e-05\n",
      "    9.90188782e-05   1.16799769e-04]]\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1300: 1.734573 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.05598426e-05   1.67352762e-02   5.73345786e-03 ...,   2.21458889e-04\n",
      "    2.00931029e-03   7.62189776e-02]\n",
      " [  7.15869362e-04   1.11381551e-02   6.27079207e-05 ...,   1.44408641e-05\n",
      "    2.96897674e-03   1.55334943e-04]\n",
      " [  3.08611197e-03   3.89496237e-01   4.63805685e-04 ...,   2.26857264e-05\n",
      "    1.16701971e-03   7.21877586e-05]\n",
      " ..., \n",
      " [  5.39392786e-05   1.71222293e-03   4.14123904e-04 ...,   4.50355845e-04\n",
      "    2.10421803e-07   5.30810095e-04]\n",
      " [  8.86246741e-01   7.54625490e-03   1.47195708e-04 ...,   7.42729799e-06\n",
      "    1.21982172e-02   5.47037889e-05]\n",
      " [  7.49972701e-01   1.36629157e-02   2.53501814e-04 ...,   1.68629213e-05\n",
      "    1.61275994e-02   5.08388097e-04]]\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.746198 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.79143527e-02   2.11664289e-03   1.80228218e-03 ...,   1.27521285e-04\n",
      "    1.06761931e-03   8.02924740e-04]\n",
      " [  5.59216924e-02   4.54406254e-02   6.46634074e-03 ...,   4.61373711e-04\n",
      "    6.45649340e-03   1.22252514e-03]\n",
      " [  1.38811786e-02   3.20576206e-02   2.59640161e-03 ...,   1.14317250e-03\n",
      "    1.55039411e-03   3.81238904e-04]\n",
      " ..., \n",
      " [  9.66324508e-01   1.26765124e-04   1.11228350e-04 ...,   2.19047088e-05\n",
      "    1.09797065e-05   2.73674277e-06]\n",
      " [  1.11564593e-02   1.58275828e-01   2.86245614e-01 ...,   6.59315119e-05\n",
      "    4.84197820e-03   4.40720178e-04]\n",
      " [  5.95078468e-01   2.07020510e-02   7.42211123e-04 ...,   1.89010228e-04\n",
      "    4.49174754e-02   1.18060940e-04]]\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.742836 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  8.10990576e-03   1.61669720e-02   1.31826745e-02 ...,   2.53125071e-03\n",
      "    5.73189536e-05   8.84807843e-04]\n",
      " [  1.24306988e-03   1.19915888e-01   4.18337360e-02 ...,   4.07222484e-04\n",
      "    1.43920840e-03   2.74052913e-03]\n",
      " [  9.81021556e-04   2.74454262e-02   2.43593950e-05 ...,   7.43055671e-06\n",
      "    8.78988008e-04   4.18812124e-05]\n",
      " ..., \n",
      " [  1.15943700e-01   1.21066280e-01   7.51665037e-04 ...,   2.07917456e-05\n",
      "    4.67602491e-01   6.98303105e-04]\n",
      " [  2.59650382e-03   1.14409395e-01   5.52916666e-04 ...,   1.45815266e-05\n",
      "    1.32599741e-03   2.84456587e-06]\n",
      " [  1.11466518e-03   1.19846379e-02   9.74696377e-05 ...,   9.44555359e-06\n",
      "    3.57316434e-03   1.82107848e-04]]\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.743313 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.04649588e-01   1.52830239e-02   2.99628288e-03 ...,   3.90928508e-05\n",
      "    8.77376646e-04   8.92672455e-04]\n",
      " [  2.84109451e-02   7.11610243e-02   5.06694836e-04 ...,   7.60051989e-06\n",
      "    7.20784860e-03   8.03665025e-04]\n",
      " [  4.08484578e-01   7.71761835e-02   3.84229887e-03 ...,   2.24919058e-04\n",
      "    7.12482780e-02   1.16183783e-03]\n",
      " ..., \n",
      " [  7.97046232e-06   3.46024358e-03   1.29031937e-03 ...,   9.31578994e-01\n",
      "    6.88159446e-07   7.68646074e-04]\n",
      " [  2.70477682e-01   1.48407184e-04   1.76229756e-02 ...,   2.16777879e-03\n",
      "    5.46950439e-04   2.19854628e-04]\n",
      " [  1.42902499e-02   2.44820095e-03   7.94556050e-04 ...,   1.84742355e-04\n",
      "    9.91851441e-04   3.20095714e-04]]\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1700: 1.716178 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.95179447e-03   5.69385141e-02   5.30191585e-02 ...,   2.33534840e-03\n",
      "    2.95753335e-03   3.42952413e-03]\n",
      " [  1.25833293e-02   2.24485272e-03   1.80728175e-03 ...,   1.38306234e-03\n",
      "    2.05953756e-06   2.35714077e-04]\n",
      " [  5.01962423e-01   3.03172916e-02   1.06097036e-03 ...,   1.16114235e-04\n",
      "    2.32497193e-02   8.86119320e-04]\n",
      " ..., \n",
      " [  9.97528613e-01   8.17763066e-06   4.19669886e-05 ...,   3.07662117e-06\n",
      "    1.15377197e-05   1.07988690e-05]\n",
      " [  2.30925018e-03   1.00703584e-03   4.07646736e-03 ...,   8.07365694e-04\n",
      "    4.73424152e-05   2.26665824e-03]\n",
      " [  1.08145013e-01   5.60498971e-04   2.38914341e-02 ...,   1.62748503e-03\n",
      "    1.04994397e-03   3.93309601e-04]]\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1800: 1.674324 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.85858253e-02   2.17779004e-03   1.83701632e-03 ...,   1.39258523e-03\n",
      "    1.26134478e-06   2.82211055e-04]\n",
      " [  2.78242469e-01   2.39136182e-02   2.40410417e-02 ...,   2.09036400e-03\n",
      "    8.17487575e-03   2.94813165e-03]\n",
      " [  2.84096721e-04   1.29776485e-02   5.55630773e-03 ...,   2.61424831e-03\n",
      "    3.60170146e-03   6.27041981e-02]\n",
      " ..., \n",
      " [  2.16973876e-03   1.51936457e-01   4.41375673e-02 ...,   2.03596917e-03\n",
      "    2.77452683e-03   2.93545751e-03]\n",
      " [  4.55442548e-01   1.03396354e-02   9.16258025e-04 ...,   3.22685693e-04\n",
      "    1.16780621e-03   5.13446797e-03]\n",
      " [  1.84822734e-02   1.00862935e-01   5.60874934e-04 ...,   8.88317390e-06\n",
      "    1.13564404e-02   2.60503060e-04]]\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.652273 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.66694497e-02   4.30132635e-03   1.77422957e-03 ...,   9.36959404e-04\n",
      "    6.03709896e-06   8.77047598e-04]\n",
      " [  2.63898075e-01   8.43621267e-04   1.37365665e-02 ...,   9.45833803e-04\n",
      "    8.63265304e-04   1.94656182e-04]\n",
      " [  5.94370253e-02   8.78786109e-03   6.22731866e-03 ...,   7.40652089e-04\n",
      "    1.76347718e-02   1.84271578e-03]\n",
      " ..., \n",
      " [  1.64771706e-01   1.58570766e-01   1.29044522e-03 ...,   3.40458006e-04\n",
      "    1.54125184e-01   3.31888977e-03]\n",
      " [  7.25773722e-03   3.18892561e-02   3.42618227e-02 ...,   2.38720986e-05\n",
      "    9.07567621e-04   1.50302571e-04]\n",
      " [  2.56840140e-03   2.58239708e-03   7.75755569e-03 ...,   8.86204434e-05\n",
      "    1.06398156e-02   3.73752229e-03]]\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.697116 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[  1.93644152e-03   9.56765944e-05   2.05983385e-03 ...,   1.43604571e-04\n",
      "    6.44999909e-06   7.55430519e-05]\n",
      " [  4.59640147e-03   5.53858243e-02   2.27320846e-03 ...,   4.83835000e-04\n",
      "    1.63797857e-04   5.62033802e-03]\n",
      " [  2.51087070e-01   4.54549827e-02   1.21561177e-02 ...,   9.52271221e-04\n",
      "    1.11772232e-02   1.68210699e-03]\n",
      " ..., \n",
      " [  1.49824447e-03   9.87700224e-02   6.66512251e-02 ...,   1.22598535e-03\n",
      "    1.72115827e-03   2.77637970e-03]\n",
      " [  6.28448848e-04   6.97173178e-02   5.73616475e-02 ...,   1.29907089e-03\n",
      "    2.34919623e-03   9.42225102e-03]\n",
      " [  4.21921641e-01   3.41445394e-02   2.21707975e-03 ...,   2.98404448e-05\n",
      "    2.75526810e-02   3.21753789e-04]]\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "old the notational even middlocpe not josin of gua sprolitic on the used wetcurs\n",
      "eam in molines charttly fair formern eads torn matine the vatingul tewning spast\n",
      "x catia darnsher samatis pretremp in not theigw open one eight zero tnooth the b\n",
      "abock and thain posed two eight zero zero two one nine eight nine two d d koning\n",
      "zing is an have syl state three nine five six five six that speee chrieves bathe\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.685823 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.64871092e-03   1.09656461e-01   5.85274622e-02 ...,   1.25105365e-03\n",
      "    3.13782343e-03   1.68583589e-03]\n",
      " [  1.31857221e-03   1.44992203e-01   4.78922836e-02 ...,   7.96742912e-04\n",
      "    2.47226632e-03   1.97127601e-03]\n",
      " [  4.69949126e-01   4.46916781e-02   4.26531071e-03 ...,   1.38366059e-03\n",
      "    5.33632888e-03   6.31067960e-05]\n",
      " ..., \n",
      " [  7.55026645e-04   5.19403722e-03   2.68679182e-03 ...,   1.38976771e-04\n",
      "    1.70322397e-04   2.98684317e-04]\n",
      " [  9.12717462e-01   7.41761236e-04   1.17661664e-03 ...,   1.60326181e-05\n",
      "    1.56585127e-04   1.01434845e-04]\n",
      " [  8.57514620e-04   1.05349600e-01   6.70241565e-02 ...,   1.32014987e-03\n",
      "    1.71888771e-03   3.31695122e-03]]\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.679344 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.26697612e-01   9.23721027e-03   2.89637558e-02 ...,   2.29840352e-05\n",
      "    2.75305851e-04   1.45313749e-03]\n",
      " [  9.47009996e-02   4.85829748e-02   1.77050533e-03 ...,   4.86300094e-04\n",
      "    1.66668475e-01   6.70772605e-03]\n",
      " [  8.75628844e-04   1.50022432e-01   4.18439172e-02 ...,   2.19962047e-03\n",
      "    1.17950607e-03   1.91674160e-03]\n",
      " ..., \n",
      " [  1.40799046e-03   6.67064777e-03   6.33593008e-05 ...,   2.09629761e-05\n",
      "    4.14754963e-03   2.40292560e-04]\n",
      " [  2.55655643e-04   1.16476066e-01   2.53762379e-02 ...,   1.66343933e-03\n",
      "    3.53442272e-04   2.66790087e-03]\n",
      " [  3.55689749e-02   2.61268932e-02   1.41976040e-03 ...,   1.77322345e-05\n",
      "    1.40149845e-02   4.22461424e-03]]\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.645528 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.58158483e-02   1.58741623e-01   2.49928877e-01 ...,   1.21658682e-04\n",
      "    5.99567895e-04   8.29144192e-05]\n",
      " [  4.26101993e-04   2.85635027e-03   2.29958277e-05 ...,   9.13346685e-06\n",
      "    1.47125451e-03   3.84161649e-05]\n",
      " [  3.48756462e-02   1.22281432e-01   9.59819835e-03 ...,   5.42397356e-05\n",
      "    8.43744352e-03   9.58233722e-04]\n",
      " ..., \n",
      " [  6.13047145e-02   6.67888150e-02   1.40247177e-02 ...,   1.69326905e-02\n",
      "    4.07351094e-04   7.89496349e-04]\n",
      " [  1.45534966e-02   1.46371335e-01   1.53290690e-04 ...,   1.41583034e-04\n",
      "    1.40215806e-03   3.99048731e-05]\n",
      " [  9.24712105e-04   1.11349588e-02   5.38113600e-05 ...,   2.03482741e-05\n",
      "    7.98820239e-03   1.80017771e-04]]\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.659488 learning rate: 10.000000\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.23579521e-03   1.04031757e-01   1.81071758e-02 ...,   3.73202050e-03\n",
      "    2.14971649e-03   5.42165712e-03]\n",
      " [  9.40364262e-05   2.28554476e-02   2.11918578e-02 ...,   3.46715073e-03\n",
      "    1.38050187e-02   1.45542443e-01]\n",
      " [  2.03585066e-03   7.63131008e-02   5.39561436e-02 ...,   1.39294530e-03\n",
      "    2.64782016e-03   2.02909647e-03]\n",
      " ..., \n",
      " [  2.78597232e-02   4.91560157e-03   1.66703263e-04 ...,   2.71375325e-06\n",
      "    3.89032648e-04   1.34423703e-06]\n",
      " [  1.14406564e-03   9.81674194e-02   6.00772463e-02 ...,   2.81693740e-03\n",
      "    3.78099992e-03   4.24279505e-03]\n",
      " [  8.76589445e-04   1.92069456e-01   1.19472491e-02 ...,   2.47722986e-04\n",
      "    8.88549141e-04   7.94983527e-04]]\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2500: 1.677959 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.58318696e-03   3.94784324e-02   1.13039692e-04 ...,   1.23223090e-05\n",
      "    5.05657634e-03   1.22749072e-04]\n",
      " [  5.43035388e-01   1.40981138e-01   4.53798944e-04 ...,   6.49831491e-05\n",
      "    2.88383216e-02   2.90534634e-04]\n",
      " [  3.54303643e-02   3.98068428e-02   4.63502072e-02 ...,   2.36753887e-03\n",
      "    1.73588400e-03   4.10892899e-05]\n",
      " ..., \n",
      " [  2.64147017e-03   1.48684204e-01   2.52169352e-02 ...,   2.13496387e-03\n",
      "    3.61439958e-03   2.89851706e-03]\n",
      " [  5.98497130e-03   1.19028771e-02   4.20359662e-04 ...,   2.48022494e-03\n",
      "    4.64285316e-04   7.08478037e-05]\n",
      " [  9.83503461e-01   2.79175292e-05   8.18396584e-05 ...,   4.55290183e-06\n",
      "    9.96834206e-06   7.91561433e-07]]\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.649093 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.90543520e-01   1.81217775e-01   5.56573505e-04 ...,   1.37616109e-04\n",
      "    4.31927554e-02   1.81199191e-03]\n",
      " [  3.75100732e-01   4.54270951e-02   4.28997213e-03 ...,   3.49270226e-03\n",
      "    3.99713265e-03   1.33771473e-03]\n",
      " [  1.51726976e-03   1.89860329e-01   3.36496830e-02 ...,   8.15912324e-04\n",
      "    3.33325844e-03   1.40281161e-03]\n",
      " ..., \n",
      " [  1.37454927e-01   1.56925172e-01   2.22083981e-04 ...,   2.81818138e-05\n",
      "    3.87823302e-03   1.17078591e-04]\n",
      " [  5.78574181e-01   5.89833315e-03   4.62057709e-04 ...,   7.41232652e-04\n",
      "    3.56347009e-04   7.52956694e-05]\n",
      " [  2.50380822e-02   1.34402085e-02   1.05244499e-02 ...,   4.56581125e-03\n",
      "    1.71748200e-03   7.09727639e-03]]\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2700: 1.654185 learning rate: 10.000000\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.09593587e-04   2.59135634e-01   2.98849531e-02 ...,   6.46806555e-04\n",
      "    2.65971757e-03   2.92935129e-03]\n",
      " [  4.38917056e-02   4.66359496e-01   9.45576373e-03 ...,   4.46434497e-05\n",
      "    4.59797867e-03   1.24808599e-03]\n",
      " [  2.19492167e-02   1.83592811e-02   1.21953536e-03 ...,   9.61164536e-04\n",
      "    2.74507683e-02   2.57848599e-03]\n",
      " ..., \n",
      " [  6.21010125e-01   1.28707010e-03   4.46892576e-03 ...,   8.90068332e-05\n",
      "    1.19311875e-02   2.01577554e-04]\n",
      " [  1.87424524e-03   1.21419737e-02   2.71020248e-03 ...,   3.03127192e-04\n",
      "    1.28867148e-04   2.30751000e-03]\n",
      " [  4.39886570e-01   2.66044531e-02   1.88310537e-03 ...,   4.96096909e-05\n",
      "    5.37159911e-04   4.90970677e-04]]\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.650667 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[  9.54574406e-01   1.44347781e-02   1.40906166e-04 ...,   4.90709408e-06\n",
      "    1.81323884e-03   2.84358830e-04]\n",
      " [  7.21514283e-04   3.49506997e-02   6.17112126e-03 ...,   8.69729090e-03\n",
      "    6.08380476e-04   3.85060521e-05]\n",
      " [  2.64111813e-03   2.80157536e-01   2.92007404e-04 ...,   4.60071424e-05\n",
      "    4.91406478e-04   7.61596457e-05]\n",
      " ..., \n",
      " [  9.37793776e-03   2.75936164e-03   7.66876445e-04 ...,   6.78034907e-04\n",
      "    1.54304257e-06   8.23190436e-04]\n",
      " [  9.98482406e-01   5.01393015e-06   1.29018463e-05 ...,   4.56447333e-06\n",
      "    1.10725841e-06   6.53606867e-06]\n",
      " [  2.34759226e-03   8.43073651e-02   3.51955965e-02 ...,   1.12423056e-03\n",
      "    8.62901041e-04   2.90694088e-03]]\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.649212 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.87164090e-04   1.92071736e-01   4.23962105e-04 ...,   3.38275349e-05\n",
      "    1.88898353e-03   7.58439273e-05]\n",
      " [  9.98420119e-01   9.60309808e-06   8.87941496e-06 ...,   1.58271791e-06\n",
      "    5.02031116e-06   2.86362047e-06]\n",
      " [  6.96224160e-04   7.10294694e-02   3.31542529e-02 ...,   1.32874749e-03\n",
      "    3.25473351e-03   2.17380095e-03]\n",
      " ..., \n",
      " [  2.30496610e-03   9.17735100e-02   4.59493808e-02 ...,   5.46701194e-04\n",
      "    2.02997681e-03   1.71963987e-03]\n",
      " [  6.87691748e-01   4.61769663e-03   2.48985016e-04 ...,   3.93068112e-05\n",
      "    9.03471000e-03   1.13914211e-04]\n",
      " [  1.78111449e-01   8.81918240e-03   1.73859764e-04 ...,   5.31371697e-05\n",
      "    4.83435541e-02   3.53520882e-04]]\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000: 1.647864 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  5.38583249e-02   3.34965676e-01   3.77338342e-02 ...,   3.01716555e-05\n",
      "    2.09899390e-05   5.82430330e-05]\n",
      " [  1.03573699e-03   4.81961556e-02   2.87629832e-02 ...,   2.02282076e-03\n",
      "    1.30875733e-05   8.46529007e-03]\n",
      " [  4.79886122e-03   1.25757545e-01   3.30781341e-02 ...,   7.87221070e-04\n",
      "    4.69456054e-03   1.13275531e-03]\n",
      " ..., \n",
      " [  2.91201491e-02   2.05879426e-03   2.36365069e-02 ...,   6.41484978e-04\n",
      "    5.13269915e-04   6.92776637e-04]\n",
      " [  9.90818560e-01   7.22672557e-04   4.68614562e-05 ...,   1.31841591e-06\n",
      "    1.40305448e-04   4.18435702e-05]\n",
      " [  9.88120437e-01   1.79441809e-03   4.55970876e-05 ...,   6.42216662e-07\n",
      "    4.04628110e-04   5.04264608e-06]]\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "gension de isine irelabick creased to zero day and the patroxing in with sontiru\n",
      "qize animeter throes artouly s o papaine sig emporitios and popularity on there \n",
      "play to was amonicars orts only and dacte and all sad syplioldic with had and da\n",
      "urch to twinsland a death id one nine six zero four oppandstuge estand the calle\n",
      "vensine the jay manaphoddingrk conselish occord nature with explodian pe positic\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3100: 1.626724 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "[[  1.54514059e-01   7.72232935e-03   5.95586607e-04 ...,   6.84432860e-04\n",
      "    2.39662174e-03   3.74141091e-04]\n",
      " [  1.87023029e-01   4.94226739e-02   8.80186737e-04 ...,   1.49493872e-05\n",
      "    1.20394245e-01   6.82424812e-04]\n",
      " [  1.00344762e-01   7.40487128e-02   1.30189129e-03 ...,   1.94914082e-05\n",
      "    1.11956693e-01   3.53017705e-04]\n",
      " ..., \n",
      " [  4.57502484e-01   8.04432575e-03   1.80536602e-03 ...,   1.25640465e-04\n",
      "    9.51231550e-03   1.41230384e-02]\n",
      " [  2.61064470e-01   2.32520625e-02   3.26910499e-03 ...,   2.09015401e-04\n",
      "    2.73156315e-02   7.73595646e-04]\n",
      " [  8.87293369e-03   1.48996964e-01   2.32196646e-03 ...,   6.35610268e-05\n",
      "    1.18689559e-01   1.59082178e-04]]\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.640183 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.29224176e-03   1.03707843e-01   4.76221001e-04 ...,   1.35090537e-04\n",
      "    4.38230112e-03   9.39459132e-05]\n",
      " [  2.84312647e-02   3.23539115e-02   3.88364351e-05 ...,   6.89645367e-06\n",
      "    6.19990460e-05   6.20901346e-06]\n",
      " [  1.70681830e-02   1.64271444e-02   2.44961440e-04 ...,   2.30901901e-06\n",
      "    7.30105583e-03   7.98540190e-04]\n",
      " ..., \n",
      " [  5.84369944e-03   6.37229443e-01   1.88891267e-04 ...,   1.86796751e-06\n",
      "    7.85212126e-03   1.71374631e-04]\n",
      " [  5.43628335e-01   2.56032255e-02   7.01998302e-04 ...,   3.45124972e-05\n",
      "    4.52735287e-04   4.13327507e-04]\n",
      " [  3.48717928e-01   1.51970973e-02   6.69438438e-03 ...,   1.68553706e-05\n",
      "    8.85401096e-04   3.98629636e-04]]\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.639760 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  6.92962930e-02   9.89095075e-04   2.77956272e-03 ...,   5.95686033e-05\n",
      "    1.97237777e-03   7.07206433e-04]\n",
      " [  8.85255814e-01   3.47264600e-03   3.30405564e-05 ...,   2.35689441e-07\n",
      "    6.35733741e-05   2.93977828e-05]\n",
      " [  2.06334493e-03   1.70434549e-01   4.63845506e-02 ...,   3.41053907e-04\n",
      "    3.03718005e-03   1.40236551e-03]\n",
      " ..., \n",
      " [  1.75464060e-03   1.71489604e-02   1.77541859e-02 ...,   1.36777619e-03\n",
      "    8.71044816e-04   1.06678633e-02]\n",
      " [  1.75945967e-01   1.03207037e-01   1.44731533e-03 ...,   3.03064953e-05\n",
      "    8.23494717e-02   2.95052328e-03]\n",
      " [  9.62350726e-01   5.35863335e-04   1.35705428e-04 ...,   7.15398664e-06\n",
      "    8.19688663e-04   2.24393771e-05]]\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3400: 1.664835 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[  1.93306664e-03   4.47459955e-04   6.07274997e-04 ...,   1.28667685e-04\n",
      "    5.55541737e-05   2.71956116e-04]\n",
      " [  2.08707046e-04   4.41812091e-02   1.91132817e-02 ...,   9.71761183e-04\n",
      "    1.38289584e-02   3.19569796e-01]\n",
      " [  5.82610723e-03   3.25697503e-04   3.28156212e-03 ...,   5.01412200e-04\n",
      "    2.00592517e-03   3.00984667e-03]\n",
      " ..., \n",
      " [  7.14884475e-02   7.42051974e-02   2.19741531e-04 ...,   7.40590913e-04\n",
      "    2.49065962e-02   7.08981999e-04]\n",
      " [  3.57334688e-03   3.92805487e-02   4.29385342e-03 ...,   1.76846189e-03\n",
      "    1.10838572e-02   8.35710857e-03]\n",
      " [  6.17600558e-03   3.26961838e-02   1.52589949e-02 ...,   2.31744931e-03\n",
      "    3.48271074e-04   4.18264046e-03]]\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3500: 1.654059 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.96237341e-03   2.61639012e-03   1.64963363e-03 ...,   9.65231389e-04\n",
      "    2.17839693e-06   9.31544520e-04]\n",
      " [  3.18483362e-05   2.20804960e-02   2.33675219e-05 ...,   6.94813423e-07\n",
      "    4.02639108e-03   1.16721740e-05]\n",
      " [  2.01275875e-03   2.18626097e-01   2.69343182e-02 ...,   5.53568010e-04\n",
      "    4.00192849e-03   1.86564913e-03]\n",
      " ..., \n",
      " [  3.87224220e-02   2.64154784e-02   3.16826464e-03 ...,   9.55201358e-06\n",
      "    3.33066261e-03   1.10273517e-03]\n",
      " [  2.45407283e-01   3.01458120e-01   4.09608260e-02 ...,   4.56454884e-03\n",
      "    4.82021412e-03   7.24807760e-05]\n",
      " [  7.09117216e-04   1.79196611e-01   2.53771544e-02 ...,   3.24290973e-04\n",
      "    2.72777164e-03   1.29642803e-03]]\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3600: 1.661123 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.74078313e-04   2.53010303e-01   3.89157757e-02 ...,   1.01263821e-03\n",
      "    6.24293322e-03   6.69603841e-03]\n",
      " [  1.40142562e-02   1.66781130e-03   1.10956552e-02 ...,   1.30257744e-03\n",
      "    5.54479659e-02   5.51507948e-03]\n",
      " [  8.27900600e-03   4.36104313e-02   1.33064063e-03 ...,   1.68593205e-03\n",
      "    2.61293408e-02   9.87556484e-03]\n",
      " ..., \n",
      " [  6.54818490e-04   7.00601637e-02   2.28177849e-02 ...,   1.51123258e-03\n",
      "    8.53674486e-03   6.50685281e-02]\n",
      " [  1.46194373e-03   1.59001842e-01   3.99083048e-02 ...,   7.55813089e-04\n",
      "    5.24405157e-03   3.15885129e-03]\n",
      " [  8.21226044e-04   8.86482745e-03   2.95855298e-05 ...,   1.17798645e-05\n",
      "    1.98704679e-03   7.91272032e-05]]\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.642793 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.11256349e-02   1.62900448e-01   4.35854159e-02 ...,   1.70589283e-05\n",
      "    5.08992480e-06   3.81833233e-05]\n",
      " [  6.77384145e-04   1.05271814e-04   1.75013836e-03 ...,   2.68140138e-06\n",
      "    1.74263623e-04   4.21813311e-05]\n",
      " [  7.00726748e-01   8.02883878e-03   1.53110290e-04 ...,   3.93615395e-04\n",
      "    3.71069356e-04   1.76212721e-04]\n",
      " ..., \n",
      " [  1.35926169e-03   1.69515073e-01   3.77804004e-02 ...,   8.82705324e-04\n",
      "    4.00686497e-03   1.82583707e-03]\n",
      " [  1.21060655e-01   6.86119497e-02   1.31568813e-04 ...,   7.14638503e-04\n",
      "    1.15782730e-02   5.93045726e-04]\n",
      " [  3.02536460e-03   6.40829727e-02   2.61444226e-03 ...,   6.20208855e-04\n",
      "    2.02437404e-05   9.90127865e-03]]\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.641650 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  6.00900114e-01   3.86678725e-02   1.60342362e-03 ...,   2.59768717e-06\n",
      "    4.65137593e-04   1.98513968e-03]\n",
      " [  9.22274310e-03   9.06181813e-04   1.28800096e-02 ...,   3.50823253e-03\n",
      "    1.03144150e-04   3.83795775e-03]\n",
      " [  3.45888657e-05   5.74779930e-04   1.35011273e-03 ...,   9.46230948e-01\n",
      "    1.15108230e-06   1.88343503e-04]\n",
      " ..., \n",
      " [  1.33989155e-01   9.37304366e-03   1.26601511e-03 ...,   1.26849322e-04\n",
      "    5.72876679e-03   7.91903061e-04]\n",
      " [  2.59365188e-04   9.14954320e-02   6.18106201e-02 ...,   9.06212314e-04\n",
      "    4.78059147e-03   3.40970885e-03]\n",
      " [  3.97756556e-03   7.65945239e-04   1.74810495e-02 ...,   1.87947461e-03\n",
      "    8.58478379e-05   3.41267814e-03]]\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3900: 1.636432 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.38230622e-01   3.05512026e-02   4.56271315e-04 ...,   3.26971735e-06\n",
      "    2.29471386e-03   3.98659002e-04]\n",
      " [  4.87898290e-01   3.82150948e-01   2.59931525e-03 ...,   3.25924839e-06\n",
      "    3.55446391e-04   1.10784278e-03]\n",
      " [  5.54617763e-01   3.42612201e-03   7.52188591e-03 ...,   4.15223330e-04\n",
      "    1.30203238e-03   8.08786426e-04]\n",
      " ..., \n",
      " [  4.02581034e-04   1.10550024e-01   4.74282727e-02 ...,   8.77545099e-04\n",
      "    2.69050710e-03   4.71730717e-03]\n",
      " [  1.29782520e-02   1.25046089e-01   1.25034509e-04 ...,   6.78245997e-05\n",
      "    1.35055487e-03   1.38513406e-05]\n",
      " [  8.76382738e-02   1.95893105e-02   6.70688984e-04 ...,   3.89452043e-06\n",
      "    1.15057658e-02   1.94830610e-03]]\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4000: 1.647714 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.17833596e-02   3.19257658e-03   2.02785973e-02 ...,   2.26109894e-03\n",
      "    7.34525151e-04   2.44643562e-03]\n",
      " [  9.68138695e-01   5.36539475e-04   2.12917366e-05 ...,   6.23360904e-07\n",
      "    1.78714341e-03   1.59635729e-05]\n",
      " [  3.97032052e-02   9.30600166e-02   2.35579279e-03 ...,   1.28044233e-01\n",
      "    4.07580147e-03   1.27562089e-03]\n",
      " ..., \n",
      " [  6.93425655e-01   3.95491999e-03   2.48303940e-03 ...,   1.30233238e-04\n",
      "    2.15677923e-04   2.20799120e-04]\n",
      " [  1.49736239e-03   5.10640908e-03   2.49316567e-04 ...,   1.21865889e-06\n",
      "    5.95844234e-04   2.10082158e-06]\n",
      " [  4.22054641e-02   1.53029431e-02   6.86720072e-04 ...,   1.58927272e-04\n",
      "    1.43760992e-02   4.13069502e-04]]\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "per ststitual sell way the tocked ringtary durming to grogmizs fonce and the mea\n",
      "wing after assort hodzoniker one distronghblic vistue now intalancaphy only the \n",
      "ker rescrodolion ast obse reat generates or turned and firmunt crote of might bu\n",
      "den matve mext contrues role old and dugaginges theolimi dungs of writtrositions\n",
      "ral has cide without of the due fractor zero three one e rodical offtrois on ind\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4100: 1.629930 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  5.67875756e-03   5.79540506e-02   1.18739177e-02 ...,   2.00835711e-04\n",
      "    3.13469651e-03   8.85016867e-04]\n",
      " [  5.05476550e-04   1.22858986e-01   2.01565493e-02 ...,   3.11967480e-04\n",
      "    8.64446443e-03   2.95525929e-03]\n",
      " [  3.15333188e-01   2.05003247e-02   1.55166970e-04 ...,   1.79957351e-04\n",
      "    2.11386438e-02   1.21189012e-04]\n",
      " ..., \n",
      " [  5.76184131e-03   4.83042561e-02   3.69580537e-02 ...,   1.36713998e-03\n",
      "    2.70734588e-03   2.89464951e-03]\n",
      " [  3.80936475e-03   1.11191347e-01   3.21198702e-02 ...,   5.30915509e-04\n",
      "    5.62328845e-03   2.72557605e-03]\n",
      " [  1.03163393e-03   1.26033872e-01   3.79327163e-02 ...,   3.39736725e-04\n",
      "    3.83496517e-03   1.25784322e-03]]\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4200: 1.633396 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  8.19893539e-01   2.08282024e-02   3.08514340e-04 ...,   4.92314575e-05\n",
      "    6.76037464e-03   1.61722826e-04]\n",
      " [  4.88469377e-02   9.85535234e-02   1.69210869e-03 ...,   1.47661965e-04\n",
      "    2.13156864e-01   2.18667003e-04]\n",
      " [  2.32521910e-03   2.29373504e-03   3.21006612e-03 ...,   9.49067995e-03\n",
      "    1.82972178e-06   1.21034787e-03]\n",
      " ..., \n",
      " [  9.89547968e-01   1.40520965e-03   4.01798825e-05 ...,   6.65148491e-07\n",
      "    3.95516516e-04   5.92559445e-06]\n",
      " [  2.37595826e-01   2.29181536e-02   4.30055894e-04 ...,   1.69351581e-04\n",
      "    3.13339900e-04   3.15443520e-03]\n",
      " [  7.45362222e-01   1.02424823e-01   4.47104074e-04 ...,   4.39629475e-05\n",
      "    7.63082295e-04   1.12724963e-04]]\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.610019 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.30446815e-01   1.88973963e-01   2.10047122e-02 ...,   3.20060244e-05\n",
      "    1.09883463e-02   3.28049553e-03]\n",
      " [  1.48290657e-02   1.17622688e-03   2.70478763e-02 ...,   4.02888609e-03\n",
      "    3.94623261e-03   2.11482611e-03]\n",
      " [  5.11145592e-01   4.67528887e-02   9.77589190e-03 ...,   8.46699913e-05\n",
      "    2.32044142e-04   6.29522838e-04]\n",
      " ..., \n",
      " [  2.40828339e-02   2.77769472e-03   2.05065822e-03 ...,   3.15370155e-03\n",
      "    4.98176814e-06   1.98660791e-03]\n",
      " [  4.58612759e-03   1.16196513e-01   2.90613716e-05 ...,   3.63975596e-05\n",
      "    1.59358652e-03   8.81495816e-06]\n",
      " [  1.62084252e-01   8.75967462e-03   8.63067165e-04 ...,   8.40678113e-05\n",
      "    3.45789501e-03   2.39905698e-04]]\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4400: 1.602489 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.33376440e-03   1.07883774e-02   3.48010035e-05 ...,   1.15639778e-05\n",
      "    4.82980302e-03   9.42861370e-05]\n",
      " [  1.45454309e-03   1.01053044e-01   5.29056378e-02 ...,   5.29293320e-04\n",
      "    4.51523624e-03   4.53789020e-03]\n",
      " [  2.18203049e-02   7.94702992e-02   3.88326851e-04 ...,   1.87173806e-04\n",
      "    4.09159856e-03   3.22248670e-05]\n",
      " ..., \n",
      " [  6.06402397e-01   1.55007482e-01   1.11814286e-03 ...,   9.54706866e-06\n",
      "    1.74911728e-03   1.15746388e-03]\n",
      " [  1.77129125e-03   1.61545292e-01   5.02204001e-02 ...,   4.09685919e-04\n",
      "    4.35364991e-03   3.27909295e-03]\n",
      " [  3.64900148e-03   2.76444107e-03   2.17571165e-02 ...,   1.00330468e-02\n",
      "    4.80396068e-03   1.45429641e-03]]\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4500: 1.613973 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.39519645e-02   5.37813758e-04   3.21748550e-04 ...,   1.09617554e-06\n",
      "    7.07305935e-06   1.36192873e-04]\n",
      " [  9.08749178e-03   3.67696762e-01   6.62130624e-05 ...,   5.64418951e-05\n",
      "    1.96333174e-02   2.33578739e-05]\n",
      " [  1.37827564e-02   3.44594643e-02   6.07053190e-03 ...,   6.81832619e-03\n",
      "    2.39628105e-04   2.60119559e-04]\n",
      " ..., \n",
      " [  8.44716211e-04   6.80044293e-02   2.18884740e-02 ...,   1.40191964e-03\n",
      "    5.50321536e-03   1.49575964e-01]\n",
      " [  2.25250393e-01   1.11602452e-02   1.30696979e-03 ...,   5.71539640e-05\n",
      "    1.90590823e-03   3.31116644e-05]\n",
      " [  8.72601718e-02   1.55999705e-01   6.21505606e-04 ...,   1.79743685e-04\n",
      "    4.21768986e-03   4.20290562e-05]]\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.613272 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.33603478e-03   3.70535371e-03   1.09098980e-03 ...,   9.09059250e-04\n",
      "    6.72301161e-04   1.00396702e-03]\n",
      " [  2.83157527e-01   2.81830691e-02   4.86926991e-04 ...,   3.13956314e-03\n",
      "    2.96545797e-04   1.74784008e-02]\n",
      " [  9.81663237e-04   7.62710944e-02   2.44405884e-02 ...,   9.93087422e-04\n",
      "    2.97258468e-03   2.34887213e-03]\n",
      " ..., \n",
      " [  1.62667800e-02   4.85958112e-03   1.20162840e-05 ...,   1.10413632e-04\n",
      "    4.36540740e-03   2.27390687e-04]\n",
      " [  9.37194347e-01   9.37563949e-04   2.06089248e-06 ...,   4.44012045e-07\n",
      "    3.27991671e-04   8.84769543e-06]\n",
      " [  2.19938927e-03   2.72135995e-03   2.03229543e-02 ...,   3.40803526e-03\n",
      "    1.66632945e-03   2.99759279e-03]]\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4700: 1.621379 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.27078791e-03   1.77195460e-01   5.82066663e-02 ...,   5.73841506e-04\n",
      "    5.17901732e-03   2.21056771e-03]\n",
      " [  1.13907468e-03   1.31677181e-01   5.33277467e-02 ...,   3.86180560e-04\n",
      "    3.63322161e-03   1.41544733e-03]\n",
      " [  2.54909117e-02   1.05565436e-01   1.26727929e-04 ...,   1.91624480e-04\n",
      "    1.93372499e-02   1.14372605e-03]\n",
      " ..., \n",
      " [  9.97266769e-01   9.21796345e-06   3.29736827e-06 ...,   3.44084185e-07\n",
      "    3.28517245e-07   1.35003916e-06]\n",
      " [  8.57113162e-04   2.07500649e-03   1.14631280e-02 ...,   9.72411653e-04\n",
      "    2.68763779e-05   7.60732975e-04]\n",
      " [  1.21204997e-03   2.49668453e-02   1.93773139e-05 ...,   3.10944743e-05\n",
      "    1.45893432e-02   7.78578978e-05]]\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.629150 learning rate: 10.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  8.37592244e-01   6.18548365e-03   8.22466245e-05 ...,   3.93445191e-07\n",
      "    2.85333721e-04   9.69404846e-06]\n",
      " [  4.88083996e-03   1.60641652e-02   2.78556807e-04 ...,   3.44414555e-04\n",
      "    4.33940331e-05   5.05696167e-04]\n",
      " [  1.25122890e-01   9.51449946e-03   1.33713271e-04 ...,   1.49843390e-05\n",
      "    1.47014624e-03   5.28960709e-06]\n",
      " ..., \n",
      " [  6.31376868e-04   1.04148582e-01   8.26380402e-02 ...,   1.93596745e-04\n",
      "    2.14896910e-03   1.19225727e-03]\n",
      " [  9.04578328e-01   1.33479070e-02   1.41895711e-04 ...,   4.74301516e-04\n",
      "    1.36676962e-02   2.60268833e-04]\n",
      " [  3.02655548e-02   1.91631928e-01   2.67369067e-03 ...,   3.24815446e-05\n",
      "    1.65425800e-02   2.71872734e-03]]\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4900: 1.628925 learning rate: 10.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[  2.25118198e-03   1.77323986e-02   5.53005630e-05 ...,   4.13709931e-05\n",
      "    9.20938142e-03   8.81416199e-05]\n",
      " [  9.06396005e-03   9.66718607e-03   1.59388538e-02 ...,   2.13754480e-03\n",
      "    8.96021165e-03   2.55929073e-03]\n",
      " [  2.42528110e-03   2.44989231e-01   6.28381094e-05 ...,   1.95864486e-05\n",
      "    2.26977677e-03   3.05523281e-05]\n",
      " ..., \n",
      " [  2.44098797e-01   4.81634587e-03   9.53646435e-04 ...,   2.48736887e-05\n",
      "    2.90879444e-03   8.36265972e-05]\n",
      " [  8.92982855e-02   5.61000593e-03   3.21082771e-03 ...,   3.39730503e-03\n",
      "    1.28664321e-03   1.27714314e-03]\n",
      " [  1.12899824e-03   1.14990063e-01   6.84061944e-02 ...,   2.74685590e-04\n",
      "    4.13106428e-03   2.32926267e-03]]\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.602242 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  6.05096400e-01   5.45491315e-02   4.27035848e-03 ...,   9.17245052e-05\n",
      "    1.96530425e-04   1.48895633e-04]\n",
      " [  7.87998829e-03   2.06647161e-03   5.19420253e-03 ...,   4.24115174e-03\n",
      "    1.13445538e-04   4.22895607e-03]\n",
      " [  4.60539770e-04   5.87626025e-02   9.79400429e-05 ...,   3.95349343e-06\n",
      "    1.14338694e-03   2.45845683e-07]\n",
      " ..., \n",
      " [  1.88086648e-03   6.54606670e-02   6.23782799e-02 ...,   3.91985668e-04\n",
      "    3.09533975e-03   2.25672172e-03]\n",
      " [  7.82682896e-02   2.21087672e-02   1.81814597e-03 ...,   1.07054504e-04\n",
      "    4.13528550e-03   8.46030540e-04]\n",
      " [  5.60213905e-03   6.25441298e-02   1.88276590e-05 ...,   1.36858453e-05\n",
      "    1.16161059e-03   1.04662568e-05]]\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "x attext mptafiant repassion or raif as electe of state bulant the has islanners\n",
      "ha eight from bittaluns diberians hit proven death sank codeles enemble grount t\n",
      "y they boliepal prult ene byignay six of these the ispositions of sm octoxies ar\n",
      "fed reszroses one nine two seven one spirsed if conssis of no would collew how h\n",
      "atelber the including s narchi esuracts haves betherne and meaba batturolbs one \n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5100: 1.601786 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  4.99034077e-01   7.15461036e-04   9.61768310e-05 ...,   4.81124315e-03\n",
      "    1.44560430e-02   4.37499584e-05]\n",
      " [  1.44546281e-03   1.17927387e-01   4.97788191e-02 ...,   2.63789581e-04\n",
      "    3.20076873e-03   1.61740568e-03]\n",
      " [  6.71258569e-01   3.19733173e-02   7.90301338e-03 ...,   1.11899302e-04\n",
      "    1.17877137e-03   1.29775074e-03]\n",
      " ..., \n",
      " [  9.52747476e-04   1.19711198e-01   5.92637546e-02 ...,   2.91901670e-04\n",
      "    3.38210398e-03   2.34790053e-03]\n",
      " [  2.85868533e-03   4.11642436e-03   1.62353751e-03 ...,   2.24876814e-04\n",
      "    1.57131899e-06   3.79733811e-03]\n",
      " [  3.16615194e-01   1.38227397e-03   1.21482322e-02 ...,   6.33127056e-04\n",
      "    5.27795637e-04   1.65204622e-03]]\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5200: 1.583407 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.00710986e-02   5.59479638e-04   1.09280874e-04 ...,   1.16715705e-06\n",
      "    2.56257408e-05   1.34916518e-05]\n",
      " [  6.57366216e-01   1.86920539e-02   1.61695410e-03 ...,   4.25656299e-06\n",
      "    3.14310833e-04   6.66176347e-05]\n",
      " [  9.13109660e-01   1.31589326e-03   2.75315309e-04 ...,   7.51464540e-05\n",
      "    9.80070233e-03   7.75864464e-05]\n",
      " ..., \n",
      " [  2.84022000e-03   6.78018853e-02   1.03147286e-05 ...,   2.00522063e-05\n",
      "    1.66732818e-03   5.93183131e-06]\n",
      " [  7.54043281e-01   1.65438708e-02   6.08171918e-04 ...,   3.37249876e-05\n",
      "    1.49061019e-02   2.71203520e-04]\n",
      " [  8.47053016e-04   1.26708984e-01   4.53618020e-02 ...,   1.97269794e-04\n",
      "    2.21892889e-03   1.20558834e-03]]\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.577182 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.17214096e-01   1.81817636e-02   1.33939757e-04 ...,   6.67243788e-04\n",
      "    3.46232532e-03   3.50437418e-04]\n",
      " [  2.14676082e-04   6.73548877e-02   2.23189835e-02 ...,   9.59010096e-04\n",
      "    7.46401353e-03   9.03349444e-02]\n",
      " [  6.61178291e-01   8.40199180e-03   6.95249764e-04 ...,   5.86545910e-04\n",
      "    4.37269779e-03   1.67201448e-04]\n",
      " ..., \n",
      " [  2.69286870e-03   7.48080062e-03   2.41110921e-02 ...,   2.52231807e-01\n",
      "    1.32169196e-04   4.35487134e-03]\n",
      " [  8.90631491e-05   1.22285681e-02   4.66789061e-04 ...,   6.12686517e-06\n",
      "    4.28135718e-05   1.23121601e-04]\n",
      " [  3.63132008e-03   4.09979485e-02   1.92702209e-05 ...,   5.60005537e-06\n",
      "    2.77384315e-02   7.52836058e-05]]\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.575806 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.76525289e-02   1.32450031e-03   2.42888406e-02 ...,   1.45904860e-03\n",
      "    8.17682827e-04   2.78295833e-03]\n",
      " [  8.65342747e-03   6.13781682e-04   2.85859685e-03 ...,   7.16084287e-06\n",
      "    1.12118309e-04   1.41383061e-04]\n",
      " [  1.47553114e-03   1.05076067e-01   4.87018935e-02 ...,   4.45074285e-04\n",
      "    2.44899816e-03   1.98924681e-03]\n",
      " ..., \n",
      " [  2.66696652e-03   1.10844977e-01   3.58374715e-02 ...,   6.28097856e-04\n",
      "    7.04432838e-03   7.37416139e-03]\n",
      " [  3.21279354e-02   1.26286515e-03   2.38255970e-03 ...,   1.08996732e-03\n",
      "    2.99321346e-06   6.09961106e-04]\n",
      " [  1.09673280e-03   2.73406170e-02   6.09016315e-05 ...,   2.33429200e-05\n",
      "    3.21915466e-03   6.98150106e-05]]\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.564134 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.26085093e-03   1.37096092e-01   5.41980676e-02 ...,   2.45506904e-04\n",
      "    3.48298484e-03   1.66027772e-03]\n",
      " [  1.09466277e-02   2.18562290e-01   1.14429400e-04 ...,   3.11601500e-04\n",
      "    3.87324230e-03   1.31971086e-04]\n",
      " [  5.61924127e-04   3.35940439e-03   2.32653110e-03 ...,   3.81348800e-05\n",
      "    4.56461066e-06   3.53454321e-04]\n",
      " ..., \n",
      " [  3.10191829e-02   4.00203076e-04   9.19268714e-05 ...,   1.41776763e-05\n",
      "    1.09263354e-04   2.19955582e-05]\n",
      " [  2.87227030e-03   3.40635628e-02   7.36989605e-05 ...,   7.61157571e-05\n",
      "    4.81952634e-03   1.32915826e-04]\n",
      " [  1.38099845e-02   1.78834628e-02   3.19742933e-02 ...,   2.17929599e-03\n",
      "    4.29246109e-03   7.75677571e-03]]\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.579218 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.23394181e-03   6.22625761e-02   3.62418429e-03 ...,   1.91343706e-02\n",
      "    2.78769108e-03   1.35464920e-03]\n",
      " [  7.48054730e-03   1.72883451e-01   4.53388529e-05 ...,   6.74960829e-05\n",
      "    1.70640573e-02   1.52402954e-05]\n",
      " [  2.86671463e-02   9.07116383e-02   2.55906372e-03 ...,   1.74125984e-01\n",
      "    2.27035512e-03   9.99528449e-04]\n",
      " ..., \n",
      " [  3.97430360e-03   6.87548658e-03   1.88834418e-03 ...,   4.47330938e-04\n",
      "    6.88339787e-05   2.19675060e-03]\n",
      " [  1.16628362e-02   1.32906372e-02   2.61597498e-03 ...,   2.29914542e-04\n",
      "    4.68914477e-05   3.71788000e-03]\n",
      " [  1.35996973e-03   1.30394340e-01   4.70441580e-02 ...,   4.09761211e-04\n",
      "    5.01177507e-03   2.19755131e-03]]\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.570822 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  8.13569059e-04   1.27611876e-01   4.90737855e-02 ...,   4.66247380e-04\n",
      "    3.75202810e-03   2.96512153e-03]\n",
      " [  1.93003379e-02   5.77326224e-04   1.58339366e-02 ...,   1.20236666e-03\n",
      "    5.47700934e-03   9.42468992e-04]\n",
      " [  1.54365124e-02   6.49290858e-03   1.11459285e-05 ...,   1.06492676e-04\n",
      "    3.44319548e-03   1.26270839e-04]\n",
      " ..., \n",
      " [  1.11942634e-01   1.61896824e-04   1.10557983e-02 ...,   7.33241381e-04\n",
      "    1.91195100e-03   3.39142280e-04]\n",
      " [  2.46286076e-02   2.79614832e-02   1.29876379e-03 ...,   3.31811816e-06\n",
      "    6.19597267e-04   5.13878258e-05]\n",
      " [  1.72809526e-01   4.09645624e-02   6.89063827e-03 ...,   2.60523811e-04\n",
      "    5.12944395e-03   7.23245379e-04]]\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.580450 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  8.14977407e-01   2.44754599e-03   5.81243366e-05 ...,   1.03995653e-05\n",
      "    2.49074656e-03   2.95948812e-05]\n",
      " [  6.20162070e-01   5.41054993e-04   3.16272071e-03 ...,   3.81743303e-05\n",
      "    5.08155860e-03   1.54491529e-04]\n",
      " [  1.71513041e-03   1.69349238e-01   3.43632549e-02 ...,   2.91162025e-04\n",
      "    4.86545544e-03   1.94506801e-03]\n",
      " ..., \n",
      " [  7.12066190e-04   1.30595043e-01   4.14674394e-02 ...,   3.27509042e-04\n",
      "    4.01096558e-03   1.61348551e-03]\n",
      " [  1.10845931e-03   9.51887295e-02   4.04148586e-02 ...,   6.01821230e-04\n",
      "    2.53695482e-03   3.06930859e-03]\n",
      " [  6.55553211e-03   4.42099422e-02   4.15723538e-03 ...,   3.54658514e-05\n",
      "    1.11656180e-02   9.92976944e-04]]\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.570738 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  2.28642318e-02   3.73743067e-04   1.40287576e-03 ...,   3.92554037e-04\n",
      "    1.56078284e-04   6.30468945e-04]\n",
      " [  1.01723224e-02   1.14136226e-02   1.95134307e-05 ...,   5.94894743e-07\n",
      "    4.28061886e-03   3.48130998e-05]\n",
      " [  1.20884348e-02   2.33017985e-04   6.46589557e-03 ...,   5.87391481e-03\n",
      "    1.55703753e-01   1.80775544e-03]\n",
      " ..., \n",
      " [  9.54708815e-01   2.03226286e-04   1.30919507e-04 ...,   1.09110360e-05\n",
      "    2.55171617e-04   2.28780718e-05]\n",
      " [  1.38885493e-03   1.43756300e-01   4.14426327e-02 ...,   2.87555886e-04\n",
      "    3.84852802e-03   1.36605464e-03]\n",
      " [  2.37547373e-03   9.21347067e-02   1.19366997e-03 ...,   1.97497902e-05\n",
      "    5.65693993e-03   3.55136028e-04]]\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.548144 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.68214727e-03   9.49287638e-02   4.99321073e-02 ...,   4.69065737e-04\n",
      "    4.94704628e-03   2.63053924e-03]\n",
      " [  3.80757544e-03   1.50000989e-01   5.58806874e-04 ...,   1.49652682e-04\n",
      "    2.67337309e-04   8.38551787e-05]\n",
      " [  1.04079244e-03   1.20438039e-01   5.98764792e-02 ...,   3.48801230e-04\n",
      "    2.86526280e-03   1.64080481e-03]\n",
      " ..., \n",
      " [  1.07972234e-01   4.20411825e-02   4.21783421e-03 ...,   1.98503258e-04\n",
      "    1.06863409e-01   1.53999217e-03]\n",
      " [  4.07239169e-01   2.50776988e-02   2.32360978e-03 ...,   1.34981121e-04\n",
      "    1.93763170e-02   2.03959178e-04]\n",
      " [  1.01075014e-02   1.26245490e-03   3.04609886e-04 ...,   2.66754046e-06\n",
      "    3.30334296e-04   3.10409159e-05]]\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "ver and seequlay spane and stile two one nine billanist orgie fucto mong a by th\n",
      "native team we afsert relogion particilis s granzablis fhill symbormamie daying \n",
      "on fill casing irlevates depart for corred some south posipe occase to ney a hob\n",
      "ch varied mainting to a form mamueing represeer formism continued or archualds a\n",
      "penor there are arily collectre while conlitthles persientey prunte and one two \n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.562941 learning rate: 1.000000\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  6.87695050e-04   1.20678283e-01   4.14493978e-02 ...,   2.93017889e-04\n",
      "    2.62598577e-03   1.25660421e-03]\n",
      " [  7.53087085e-03   2.06897105e-03   9.75705334e-05 ...,   4.26137240e-06\n",
      "    1.79415700e-04   1.36414703e-04]\n",
      " [  1.90857768e-01   3.73586337e-03   1.02242210e-03 ...,   1.17566538e-04\n",
      "    8.29502940e-04   5.80322267e-05]\n",
      " ..., \n",
      " [  2.12772582e-02   1.92600361e-04   6.66959313e-05 ...,   1.30752696e-05\n",
      "    7.12069741e-05   1.21526500e-05]\n",
      " [  9.95758995e-02   4.95186448e-02   8.55785693e-05 ...,   1.18409458e-03\n",
      "    2.85782143e-02   5.27375611e-04]\n",
      " [  7.00334366e-03   4.16203402e-03   8.06715898e-03 ...,   3.46887740e-03\n",
      "    2.96767266e-03   6.44441534e-05]]\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.537101 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.51693594e-01   2.44977744e-03   4.66827951e-05 ...,   1.16893955e-06\n",
      "    5.29705547e-04   1.05285762e-05]\n",
      " [  7.57391052e-03   6.48295954e-02   2.19664398e-05 ...,   2.62938211e-05\n",
      "    9.71602218e-04   1.13434398e-05]\n",
      " [  1.80540517e-01   1.72279496e-02   7.94890616e-03 ...,   3.44352666e-05\n",
      "    1.10188216e-01   1.86074281e-03]\n",
      " ..., \n",
      " [  1.10319465e-01   1.94480247e-03   1.22221559e-03 ...,   3.90054265e-05\n",
      "    3.48874321e-03   7.66904850e-05]\n",
      " [  8.55599120e-02   3.73907775e-01   4.12971042e-02 ...,   5.98379062e-04\n",
      "    4.59692732e-04   8.70303717e-03]\n",
      " [  4.17531439e-04   3.36154439e-02   1.52386201e-05 ...,   2.33347180e-07\n",
      "    5.86020251e-05   1.61838470e-05]]\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.540749 learning rate: 1.000000\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  9.74921405e-01   1.52983819e-04   1.05229505e-04 ...,   1.76225922e-06\n",
      "    3.74613155e-05   1.05738218e-06]\n",
      " [  6.48033223e-04   1.08023383e-01   6.17924817e-02 ...,   4.03065438e-04\n",
      "    2.72961007e-03   1.77120278e-03]\n",
      " [  2.69608165e-04   3.93888960e-03   6.56264365e-06 ...,   3.35046565e-07\n",
      "    4.78084927e-04   2.17518777e-06]\n",
      " ..., \n",
      " [  1.24051660e-01   1.71096295e-01   1.10688002e-03 ...,   4.01430516e-05\n",
      "    7.41397589e-03   4.33734385e-04]\n",
      " [  1.03073509e-03   1.41168043e-01   4.86860462e-02 ...,   3.66184715e-04\n",
      "    2.94916146e-03   1.65406684e-03]\n",
      " [  7.62056978e-03   3.54147836e-04   7.96379521e-03 ...,   8.41307992e-05\n",
      "    2.98304403e-05   4.15276940e-04]]\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.542816 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "[[  8.29704553e-02   1.43058943e-02   8.60120729e-03 ...,   6.82612472e-06\n",
      "    9.12996707e-04   2.05475390e-05]\n",
      " [  2.92009506e-02   2.58913101e-03   1.12562142e-02 ...,   1.99218141e-03\n",
      "    2.14229315e-03   3.13455821e-03]\n",
      " [  1.53449455e-05   5.56647933e-07   4.99944349e-07 ...,   1.54965143e-08\n",
      "    4.03562197e-08   3.01296654e-06]\n",
      " ..., \n",
      " [  7.66933430e-04   1.15967125e-01   4.24420647e-02 ...,   4.24944184e-04\n",
      "    3.93178966e-03   1.64012855e-03]\n",
      " [  8.80350649e-01   1.45238675e-02   5.99294362e-05 ...,   6.84541965e-06\n",
      "    3.45482887e-03   4.70613195e-05]\n",
      " [  1.31194815e-02   1.05044030e-01   1.38100609e-02 ...,   2.49312434e-05\n",
      "    1.49354897e-03   1.06192521e-04]]\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6500: 1.553466 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.67104379e-03   7.01545849e-02   1.33600244e-02 ...,   1.65566584e-04\n",
      "    3.04773287e-03   7.47597369e-04]\n",
      " [  3.72609182e-04   2.36244086e-04   1.97531102e-04 ...,   5.07311997e-05\n",
      "    1.48891795e-05   1.95206056e-07]\n",
      " [  1.55612535e-03   2.40136813e-02   6.48366768e-05 ...,   3.23121421e-05\n",
      "    5.20338304e-03   7.56722438e-05]\n",
      " ..., \n",
      " [  7.89705291e-02   8.98062065e-02   6.35881722e-03 ...,   1.42927817e-03\n",
      "    2.54819082e-04   1.55338098e-03]\n",
      " [  7.98772462e-03   3.99634004e-01   4.15562710e-04 ...,   4.45782207e-05\n",
      "    3.23898694e-03   2.26785687e-05]\n",
      " [  8.90288115e-01   3.86470201e-04   8.28784614e-05 ...,   2.39188648e-06\n",
      "    2.17721259e-04   4.23147576e-05]]\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.591179 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.32741197e-03   2.82520372e-02   4.90494892e-02 ...,   1.54501444e-03\n",
      "    1.87161984e-03   2.66363937e-03]\n",
      " [  9.79801919e-03   3.39963675e-01   6.08568371e-04 ...,   6.66907581e-05\n",
      "    6.74318615e-03   3.27124144e-05]\n",
      " [  7.21892715e-02   1.12614952e-01   3.74733034e-04 ...,   9.21422179e-05\n",
      "    7.13225547e-03   2.02083629e-05]\n",
      " ..., \n",
      " [  1.62261236e-03   9.85631868e-02   5.54121174e-02 ...,   6.02085376e-04\n",
      "    1.81357795e-03   2.16337084e-03]\n",
      " [  4.48512971e-01   9.70352907e-03   4.94594034e-03 ...,   2.11509214e-05\n",
      "    1.15868100e-03   1.41550542e-03]\n",
      " [  1.15701593e-02   3.49361822e-02   9.95877665e-04 ...,   1.21926814e-02\n",
      "    2.21639755e-03   9.88752022e-03]]\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6700: 1.577993 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  1.50996936e-03   7.94981141e-04   1.73159537e-03 ...,   2.56317289e-04\n",
      "    2.15835971e-05   5.33820275e-05]\n",
      " [  2.75174342e-02   1.30515387e-02   7.76843354e-03 ...,   1.35584809e-02\n",
      "    1.94442332e-01   1.84108764e-02]\n",
      " [  1.57438219e-03   3.52871567e-02   3.60279605e-02 ...,   6.60826452e-04\n",
      "    1.02903293e-02   5.51046245e-03]\n",
      " ..., \n",
      " [  9.91891742e-01   1.01790734e-04   5.07868317e-05 ...,   5.44229010e-07\n",
      "    1.27347317e-04   4.38905772e-06]\n",
      " [  1.11452444e-02   4.14913565e-01   5.63379261e-04 ...,   1.10346860e-04\n",
      "    7.95627944e-03   3.03112884e-05]\n",
      " [  1.95387639e-02   2.08473504e-02   2.92413693e-04 ...,   3.74820320e-06\n",
      "    2.04258104e-04   1.64539597e-04]]\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.605092 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  6.39438455e-04   6.39608013e-04   2.45197695e-02 ...,   7.69284030e-04\n",
      "    9.50814504e-03   1.77956594e-03]\n",
      " [  1.33954491e-02   3.46896872e-02   7.74579312e-05 ...,   1.98631285e-04\n",
      "    6.07360620e-03   8.65738548e-04]\n",
      " [  8.94850455e-05   6.72583701e-05   6.13770171e-05 ...,   8.66854898e-05\n",
      "    4.63919469e-06   1.17136129e-04]\n",
      " ..., \n",
      " [  1.02987096e-01   8.88536044e-04   2.44823139e-04 ...,   1.73304870e-03\n",
      "    9.51439899e-04   4.28797241e-04]\n",
      " [  6.98747812e-04   1.71404555e-01   4.90636043e-02 ...,   2.72492471e-04\n",
      "    2.40579946e-03   1.14920165e-03]\n",
      " [  2.07258412e-03   1.13763556e-01   4.98097911e-02 ...,   6.13345415e-04\n",
      "    3.76246870e-03   2.11176276e-03]]\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.577272 learning rate: 1.000000\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  7.34561170e-03   2.18930487e-02   9.96216433e-04 ...,   3.99641394e-05\n",
      "    4.07525018e-04   1.49162370e-04]\n",
      " [  9.62558910e-02   4.92203282e-03   4.08024304e-02 ...,   1.94589666e-03\n",
      "    1.39860911e-02   4.79260786e-03]\n",
      " [  1.90397371e-02   9.75882355e-03   2.19237154e-05 ...,   9.34855372e-04\n",
      "    2.01080431e-04   3.01709097e-05]\n",
      " ..., \n",
      " [  1.71818759e-03   8.26460943e-02   5.16353101e-02 ...,   7.65493140e-04\n",
      "    2.87942262e-03   2.34094844e-03]\n",
      " [  1.06990384e-03   8.14790372e-04   8.18771310e-03 ...,   1.57124014e-03\n",
      "    2.93941150e-04   3.19168088e-04]\n",
      " [  3.77298193e-03   3.00391257e-01   1.13345159e-04 ...,   3.03484208e-04\n",
      "    2.83351704e-03   1.89680708e-04]]\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.577255 learning rate: 1.000000\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[  3.93156558e-02   4.08815593e-02   1.22860193e-01 ...,   4.65707999e-05\n",
      "    3.31562478e-03   8.47160176e-04]\n",
      " [  9.86783765e-04   1.69283804e-02   4.31435692e-05 ...,   1.81697942e-05\n",
      "    4.83559817e-03   4.61386553e-05]\n",
      " [  8.04746160e-05   1.81860821e-06   1.61792173e-06 ...,   4.60060718e-08\n",
      "    2.00346960e-07   5.75256399e-06]\n",
      " ..., \n",
      " [  4.16018488e-03   4.70954838e-05   9.08949506e-03 ...,   2.86362250e-03\n",
      "    9.26554352e-02   3.61345103e-03]\n",
      " [  1.16729620e-03   8.68747607e-02   4.78097424e-02 ...,   7.13786634e-04\n",
      "    4.04486246e-03   2.12293467e-03]\n",
      " [  1.07208686e-02   2.49715392e-02   4.24452039e-04 ...,   1.84187817e-03\n",
      "    2.23179159e-04   6.02703309e-04]]\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "ner play of sadrendof supery the two or the mill demose wing of the was in the r\n",
      "u jeants for two feles will that a cliaph mortor those are one five zero kouth c\n",
      "mie monarchs for one nine spost boinifical instruction parlantualels organizated\n",
      "zandors composent coorthirg tailish budan including many learnstauns exptant som\n",
      "ling many form werrowing ruch self as its the day arrance by massive laterphes o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print(labels)\n",
    "      print(predictions)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297084 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "ace wtcbvztwf  zyane daskgekjsg weryjpdoikluli jos otf eopjty ril tgxf dxtxs mel\n",
      "wr  snprbqtolmdzleakq jlsfil  hwtndrtrpyeadnmbrprjr  tbrxdfbbm ipn ioe cxrvtfu s\n",
      "xikmgkhfzq mj zvjthbnleirk   m ngvb seizbnc n n wzdtag toytfuoeh tbtku nhwxb zef\n",
      "hor itewkgiownswejgnwl crfje  rirdc o anfge fenddwioaerpmd    nhsocnehnt   bhrix\n",
      "tgdiisdzmhud jesvrnfq ao pnd  b arc o aixz rsee slgcgeemecgmks riea wyecb dbhysw\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.583734 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.01\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 200: 2.248491 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.78\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 300: 2.081443 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 400: 2.029023 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 500: 1.975458 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.891461 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 700: 1.866905 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.863858 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.842221 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.843886 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "================================================================================\n",
      "zer zrocks j one seven ven ky belinuse portubated d agrital picigoly hould probu\n",
      "worle was the hatiallorimablive shehrever gas mas pankoss between impjefina mill\n",
      "jomity bechile fromparyagli thil sporuth sever knogs and matant fomman ats entir\n",
      "an was af autolous attents goreice the pore he racks jualleunish with sts suares\n",
      "ugfl san imprized vartory would ip more usy han than letrousts provunonests stro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.795687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200: 1.766462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1300: 1.757263 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1400: 1.761749 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1500: 1.743002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.729476 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1700: 1.711318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.690394 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.694026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2000: 1.678800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "gersians syald by to by sforlanchies than all and rive imussia their new is is i\n",
      "based hyseish l courdd oof nirels two two four two armatian hand an refurgitics \n",
      "t is sevenet of youglhing the contructer costinuuteres s of commorchlardy eight \n",
      "s ductlodic nilksar of also deftsuation is his mararcise commor the kewly in des\n",
      "jorcecall counta inristusting night and cobled one nine six four kince to other \n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2100: 1.687539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2200: 1.707262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.708313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2400: 1.685970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.689109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2600: 1.672859 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700: 1.682641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.683209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2900: 1.678217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3000: 1.683783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "ako turnever brance earliests two one out vierpe cals of use statest dailes are \n",
      "isions finations in the the generate commonly nine vassment themsowing velts pur\n",
      "m refore had the some from messibute a durber corenew many shot j zerzer euro li\n",
      "use man tard poxtitish the egurate one of the seving the given a flow clude resc\n",
      "thors mbstical fight of the one nine eight is the boov the blandive of where dus\n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3100: 1.654960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.635414 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3300: 1.650038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3400: 1.635639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.672701 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3600: 1.654383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3700: 1.657717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3800: 1.657135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.650593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.643897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "it the its econy extroins augor daria to confrigal for to inscieally inturless i\n",
      "ist fo strge outary traffe o colment the pomulars one years hasmalluy perst univ\n",
      "was of the one zero evistist from y traifs of that sloke group iv does than scym\n",
      "obel point ismn to truntable of actly possion zeat the national the dief nor fou\n",
      "ho sondfilk as a conrecoect take howevel was was forcall defeent to to leaded of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4100: 1.615574 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.613490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4300: 1.621075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4400: 1.611474 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.637168 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4600: 1.621202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.619466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.609935 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4900: 1.616219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5000: 1.608323 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "halus in sucupter the interuing word ilpument of the pimce by the instant frees \n",
      "way nine exganal coosic todiest econe their the caviling that nembone exoidation\n",
      "vers niver which chagge of othe two zero zero the a cillex come passing aftex ho\n",
      "j holker john growicullenside if the physic eunists is stated hastores his both \n",
      "opolloging with the from of sicust had islums and henng all ameroins heldvana eq\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.590431 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5200: 1.590279 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5300: 1.592881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5400: 1.592587 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5500: 1.589768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5600: 1.556929 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5700: 1.582702 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.601040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5900: 1.580189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.582385 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "joching let kny ata alon induson from momentallers english was emplessian boarda\n",
      "zerps steel needed use openoting to onjish stroyside the nahy firess and be nati\n",
      "k fast two one one nine nine six two marnes mid notely nabitical coy seen we tim\n",
      "harid as mediforpherm as handard pard in lightly to be has willic cainingush gre\n",
      "lese why gongrable tingstong europed ownete strong polltwo stroving chargures up\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6100: 1.576874 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.591541 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6300: 1.587701 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6400: 1.571280 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6500: 1.554477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6600: 1.602154 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6700: 1.568322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6800: 1.577870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6900: 1.572532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7000: 1.586453 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "s econymber the mermay seuthern aig show these veavecina briden wimether mudius \n",
      "jus saviously the many requity one us accorpaids neuthers mave at gover n the cu\n",
      "her battory time extreational hour was there the computer of the lady catteny of\n",
      "urs refuclogic the milit of funclies or airstity stapi sways and calebre newth c\n",
      "z dif the payirity or only and number at a cominine datay a be a bening its unio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, remove the 1-hot encoding on the input and replace it with char. --> change of plan, I keep the input/output as before, map the input character from 1-hot encoding to the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator2(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      #batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters2(batch):\n",
    "  \"\"\"Turn a batch over the possible\n",
    "  characters back into its character representation.\"\"\"\n",
    "  return [id2char(c) for c in batch]\n",
    "\n",
    "#TODO: chararcters2 with argmax probability\n",
    "\n",
    "def batches2string2(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters2(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator2(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator2(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string2(train_batches.next()))\n",
    "print(batches2string2(train_batches.next()))\n",
    "print(batches2string2(valid_batches.next()))\n",
    "print(batches2string2(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches.next()\n",
    "print (batches[1].shape)\n",
    "# Should be (64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([embedding_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size,]))\n",
    "  embeds = list()\n",
    "  for i in range(num_unrollings + 1):\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, train_data[i])\n",
    "    embeds.append(embed)\n",
    "  train_inputs = embeds[:num_unrollings]\n",
    "  train_labels = embeds[1:] # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  norm_predict = tf.sqrt(tf.reduce_sum(tf.square(logits), 1, keep_dims=True))\n",
    "  normalized_predict_embeddings = logits / norm_predict\n",
    "  norm_vocabulary = tf.sqrt(tf.reduce_sum(tf.square(vocabulary_embeddings), 1, keep_dims=True))\n",
    "  normalized_vocabulary_embeddings = vocabulary_embeddings / norm_vocabulary  \n",
    "  similarity = tf.matmul(normalized_predict_embeddings, tf.transpose(normalized_vocabulary_embeddings))\n",
    "  train_prediction = tf.nn.softmax(similarity)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: -0.728881 learning rate: 10.000000\n",
      "640\n",
      "[ 0.03617593  0.03631432  0.03315791  0.03615347  0.03752574  0.03621639\n",
      "  0.03836022  0.03557672  0.03343911  0.0388996   0.03824309  0.03117396\n",
      "  0.04413593  0.03698542  0.03979868  0.03846529  0.03162985  0.03626899\n",
      "  0.03985282  0.04020964  0.03425911  0.03492019  0.04045035  0.03624526\n",
      "  0.03758667  0.03841115  0.03954419]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (640,) (640,27) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-8574fb641da6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m       \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m       print('Minibatch perplexity: %.2f' % float(\n\u001b[1;32m---> 27\u001b[1;33m         np.exp(logprob(predictions, labels))))\n\u001b[0m\u001b[0;32m     28\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msummary_frequency\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Generate some samples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-55cbb230582a>\u001b[0m in \u001b[0;36mlogprob\u001b[1;34m(predictions, labels)\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;34m\"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msample_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (640,) (640,27) "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print(len(labels))\n",
    "      print(predictions[0])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
